# -*- coding: utf-8 -*-
"""Vidio_Youtube_coment_Purbaya Sadewa X Gita Wirjawan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yGyHYe_2OsNbE6-22VEZL6HxY9SeLIJT
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install youtube-comment-downloader

from youtube_comment_downloader import YoutubeCommentDownloader
import pandas as pd

downloader = YoutubeCommentDownloader()
comments = []

url = "https://www.youtube.com/watch?v=Ss6ud71pmvQ"  # ganti dengan video kamu
# Vidio/Official iNews/LIVE |Borok Polisi Dibongkar, Mampukah Reformasi Polri Jawab Harapan Publik? | Rakyat Bersuara

# sort_by = 0 (Top comments), 1 (Newest first)
for comment in downloader.get_comments_from_url(url, sort_by=0):
    comments.append({
        'user': comment.get('author'),
        'comment': comment.get('text'),
        'time': comment.get('time'),
        'votes': comment.get('votes')
    })

df = pd.DataFrame(comments)
print("Jumlah komentar:", len(df))
df.head()

# # Menyimpan data Frame
# df = pd.DataFrame(comments)
# df.to_csv("youtube_comments_Purbaya X Gita Wirjawan.csv", index=False, encoding='utf-8')
# print("Komentar berhasil disimpan ke 'youtube_comments.csv'")

import pandas as pd
# Tentukan path file CSV
path = "/content/drive/MyDrive/Semester7 2026/Pemrosesan Teks/Analisis Sentimen Comment YouTube Vidio Purbaya(Menku) X Gita Wirjawan/youtube_comments_Purbaya X Gita Wirjawan.csv"
# Membaca data CSV ke dalam DataFrame
df = pd.read_csv(path, encoding='utf-8')
# Menampilkan beberapa baris pertama
print(df.head())

# menampilkan dataset yang ada pada data comment
comment = df["comment"]
print(comment.head())

"""Langkah selanjutnya melakukan Preprosesing text pada coloum text

"""

import re
from wordcloud import WordCloud
import nltk
import requests
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

"""

1.   Cleaning data dan Case Folding

"""

import re
# Pastikan kolom 'comment' ada
if 'comment' in df.columns:
    # Fungsi cleaning & case folding
    def clean_text(text):
        if isinstance(text, str):
            text = text.lower()                             # Case folding (ubah ke huruf kecil)
            text = re.sub(r"http\S+|www\S+|https\S+", '', text)  # Hapus URL
            text = re.sub(r'@\w+|#\w+', '', text)           # Hapus mention dan hashtag
            text = re.sub(r'[^a-zA-Z0-9\s]', '', text)      # Hapus karakter non-alfanumerik
            text = re.sub(r'\s+', ' ', text).strip()        # Hapus spasi berlebih
            return text
        else:
            return ''

    # Terapkan fungsi ke kolom 'comment'
    df['Case Folding'] = df['comment'].apply(clean_text)

    # Lihat hasil
    print(df[['comment', 'Case Folding']].head())
else:
    print("Kolom 'comment' tidak ditemukan di DataFrame.")

"""2.   Filtering (symbol, angka, hastag, mention,
whitespace, stopword removal)


"""

! pip install Sastrawi

from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
# Inisialisasi stopword remover (Bahasa Indonesia)
factory = StopWordRemoverFactory()
stopword = factory.create_stop_word_remover()

# Fungsi cleaning dan filtering
def stop_word_remover(text):
    if isinstance(text, str):
        text = text.lower()  # Case folding

        # Hapus URL
        text = re.sub(r'http\S+|www\S+', '', text)

        # Hapus mention & hashtag
        text = re.sub(r'@\w+|#\w+', '', text)

        # Hapus angka
        text = re.sub(r'\d+', '', text)

        # Hapus simbol dan karakter non-alfabet
        text = re.sub(r'[^a-z\s]', '', text)

        # Hapus whitespace berlebih
        text = re.sub(r'\s+', ' ', text).strip()

        # Hapus stopword
        text = stopword.remove(text)

        return text
    else:
        return ''

# Terapkan pada kolom 'comment'
df['stopword removal'] = df['Case Folding'].apply(stop_word_remover)

# Tampilkan hasil
print(df[['Case Folding', 'stopword removal']].head())

"""

3.   Tokenizing (konversi kata didalam kalimat menjadi
bentuk tabel/kolom) dan Lemmatization

"""

!pip install langdetect

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import words
from nltk.stem import WordNetLemmatizer
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from langdetect import detect

# Unduh resource NLTK (pertama kali saja)
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('words')

# Inisialisasi tools
english_words = set(words.words())
lemmatizer_en = WordNetLemmatizer()
stem_factory = StemmerFactory()
stemmer_id = stem_factory.create_stemmer()
stop_factory = StopWordRemoverFactory()
stopword = stop_factory.create_stop_word_remover()

# Fungsi Preprocessing Lengkap
def preprocess_bilingual(text):
    if not isinstance(text, str):
        return ''
     # Tokenizing
    tokens = word_tokenize(text)

    # Lemmatization / Stemming bilingual
    processed_tokens = []
    for token in tokens:
        try:
            lang = detect(token)  # deteksi bahasa per kata
        except:
            lang = "id"  # default ke Indonesia jika tidak terdeteksi

        if lang == 'en' and token in english_words:
            token = lemmatizer_en.lemmatize(token)  # Lemmatization English
        else:
            token = stemmer_id.stem(token)          # Stemming Indonesia

        processed_tokens.append(token)

    return ' '.join(processed_tokens)

df['Lemmatization'] = df['stopword removal'].apply(preprocess_bilingual)

# Token kolom
df['tokens'] = df['Lemmatization'].apply(word_tokenize)

print(df[['stopword removal', 'Lemmatization', 'tokens']].head())

"""

4.   Stemming (merubah jadi kata dasar) dari colum stopword removal



"""

from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

# Inisialisasi
stop_factory = StopWordRemoverFactory()
stopword = stop_factory.create_stop_word_remover()

stem_factory = StemmerFactory()
stemmer = stem_factory.create_stemmer()

# Fungsi preprocessing
def preprocess_text(tokens):
    if isinstance(tokens, list):  # <-- cek jika token berupa list
        text = ' '.join(tokens)   # ubah list -> string
        text = stopword.remove(text)
        text = stemmer.stem(text)
        return text
    elif isinstance(tokens, str):  # kalau sudah string
        text = stopword.remove(tokens)
        text = stemmer.stem(text)
        return text
    else:
        return ''

# Terapkan ke kolom 'tokens'
df['stemming'] = df['tokens'].apply(preprocess_text)

# Lihat hasil
print(df[['tokens', 'stemming']].head())

"""! pip install transformer

transformers adalah library open-source buatan perusahaan Hugging Face, yang menyediakan model-model kecerdasan buatan (AI) modern untuk memahami dan menghasilkan bahasa manusia, gambar, suara, bahkan kode.


*Perusahaan Pembuat   
Hugging Face Inc., perusahaan AI asal Paris, Prancis (sekarang juga berbasis di New York).

Mereka fokus pada:

Natural Language Processing (NLP)

Generative AI

Open model ecosystem (seperti BERT, GPT, T5, RoBERTa, DistilBERT, dll.)
*   Fungsinya
| Jenis Tugas                   | Fungsi di `transformers`                   | Contoh Model     |
| ----------------------------- | ------------------------------------------ | ---------------- |
| **Analisis Sentimen**         | `pipeline("sentiment-analysis")`           | BERT, DistilBERT |
| **Penerjemahan Bahasa**       | `pipeline("translation")`                  | T5, MarianMT     |
| **Ringkasan Teks**            | `pipeline("summarization")`                | BART, Pegasus    |
| **Jawaban dari Teks (QA)**    | `pipeline("question-answering")`           | RoBERTa, BERT    |
| **Chatbot / Text Generation** | `pipeline("text-generation")`              | GPT-2, GPT-Neo   |
| **Analisis Gambar (Vision)**  | `pipeline("image-classification")`         | ViT, CLIP        |
| **Speech / Audio**            | `pipeline("automatic-speech-recognition")` | Whisper          |

Ringkasan Otomatis

summarizer = pipeline("summarization")
text = "Artificial Intelligence (AI) has revolutionized many industries..."
print(summarizer(text, max_length=40, min_length=10))

Penerjemahan Bahasa

translator = pipeline("translation_en_to_id")
print(translator("Hugging Face is a leading company in natural language processing."))

5.   Labeling Mengginakan library tranformer
"""

from transformers import pipeline

# Inisialisasi model sentiment analysis
# analyzer = pipeline("sentiment-analysis")

# Memakai model sentimen Bahasa Indonesia,
analyzer = pipeline("sentiment-analysis", model="w11wo/indonesian-roberta-base-sentiment-classifier")

# Terapkan analisis sentimen ke setiap baris kolom 'stemming'
results = df['stemming'].apply(lambda x: analyzer(x)[0] if isinstance(x, str) and x.strip() != "" else {'label': 'UNKNOWN', 'score': 0})

# Tambahkan hasil ke kolom baru
df['sentiment_label'] = results.apply(lambda x: x['label'])
df['sentiment_score'] = results.apply(lambda x: x['score'])

# Tampilkan contoh hasil
print(df[['stemming', 'sentiment_label', 'sentiment_score']].head())

print(df.head())

sentiment_count = df['sentiment_label'].value_counts()
print(sentiment_count)

# Plot distribusi label
plt.figure(figsize=(6, 4))
sentiment_count.plot(kind='bar', color=['red', 'gray', 'green'])

plt.title('Distribusi Label Sentimen')
plt.xlabel('sentimen_label')
plt.ylabel('Jumlah Data')
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Menampilkan semua data dengan label 'UNKNOWN'
unknown_reviews = df[df['sentiment_label'] == 'UNKNOWN']

# Melihat jumlahnya untuk konfirmasi
print(f"Jumlah data UNKNOWN: {len(unknown_reviews)}")

# Menampilkan beberapa contoh review
print(unknown_reviews.head(47))  # tampilkan 10 review pertama

# Hapus baris dengan label 'UNKNOWN'
df = df[df['sentiment_label'] != 'UNKNOWN']

# Reset ulang index agar rapi
df = df.reset_index(drop=True)

# Cek ulang distribusi setelah penghapusan
print(df['sentiment_label'].value_counts())

# Hitung jumlah label setelah drop
sentiment_count = df['sentiment_label'].value_counts()
# Plot distribusi label
plt.figure(figsize=(6, 4))
sentiment_count.plot(kind='bar', color=['green', 'red', 'gray'])
plt.title('Distribusi Label Sentimen', fontsize=14)
plt.xlabel('sentimen_label')
plt.ylabel('Jumlah Data')
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.show()

"""Labeling Encoding"""

dict_label_sentimen = {}
label_classes = []
index = 0
for label in df['sentiment_label']:
  if label not in dict_label_sentimen:
    dict_label_sentimen[label] = index
    label_classes.append(label)
    index += 1

label_order = ['negative', 'neutral', 'positive']
dict_label_sentimen = {label: i for i, label in enumerate(label_order)}

df['sentiment_encoded'] = df['sentiment_label'].apply(lambda x: dict_label_sentimen[x])

dict_label_sentimen

df

"""Pembagian dataset"""

X = df['stemming'].to_numpy()
y = df['sentiment_encoded'].to_numpy()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

sentiment_counts = {
    'Positive': np.array([0, 0]),
    'Negative': np.array([0, 0]),
    'Neutral': np.array([0, 0])
}

sentiment_counts = {label: [0, 0] for label in label_classes}

for label in y_train:
    sentiment_counts[label_classes[label]][0] += 1

for label in y_test:
    sentiment_counts[label_classes[label]][1] += 1

sentiment_counts[label_classes[label]]

fig, ax = plt.subplots(figsize=(8,6))

bottom = np.zeros(2)
index_color = 0

data_horizontal = ('Data Latih', 'Data Uji')
bar_colors = ['tab:blue', 'tab:red', 'tab:gray']
width = 0.8  # the width of the bars: can also be len(x) sequence

for label, jumlah_sentimen in sentiment_counts.items():
    p = ax.bar(data_horizontal, jumlah_sentimen, width, label=label, bottom=bottom, color=bar_colors[index_color])

    if label == "Neutral":
        # tampilkan di bagian atas (edge) dengan sedikit jarak
        ax.bar_label(p, labels=[f"{v:.0f}" for v in jumlah_sentimen],
                     label_type='edge', padding=5, color='black', size=18)
    else:
        ax.bar_label(p, labels=[f"{v:.0f}" for v in jumlah_sentimen],
                     label_type='center', color='black', size=18)

    bottom += jumlah_sentimen
    index_color += 1

ax.set_title('Jumlah Data Latih dan Uji', fontsize=16)
ax.legend()
ax.set_ylim(0, 1500)
plt.show()

"""Ekstrasi Fitur (TF-IDF)

"""

tfidf = TfidfVectorizer(
    tokenizer=lambda x: x,
    preprocessor=lambda x: x,
    token_pattern=None
)

tfidf.fit(X_train)

X_train = tfidf.transform(X_train)
X_test = tfidf.transform(X_test)

"""# Klasifikasi dengan menggunakan Decision Tree

Training
"""

from sklearn.tree import DecisionTreeClassifier, export_graphviz

model_decision_tree = DecisionTreeClassifier(
    criterion="entropy", random_state=0, max_depth=1, min_samples_split = 12
    )

model_decision_tree.fit(X_train, y_train)

"""Testing dan Hasil evaluasi

"""

y_pred = model_decision_tree.predict(X_test)

y_pred

y_test

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
accuracy_score(y_test, y_pred)

# Evaluasi
print("Akurasi:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n")
print(classification_report(y_test, y_pred, target_names=label_classes))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,6))
sns.heatmap(cm, annot=True, fmt='d',
            xticklabels=label_classes,
            yticklabels=label_classes,
            cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

tfidf.get_feature_names_out()

nama_fitur = tfidf.get_feature_names_out()

dot_data = export_graphviz(model_decision_tree, out_file=None, feature_names=nama_fitur,
                           class_names=np.array(label_classes))

import pydotplus
from IPython.display import Image

graph = pydotplus.graph_from_dot_data(dot_data)
Image(graph.create_png())
graph.write_png("Sentimen Ulasan_youtube_comments_Purbaya X Gita Wirjawan.png")

"""Eksperimen"""

max_depth_list = [i for i in range(1, 20)]
min_samples_split_list = [i for i in range(1, 20)]
random_state_list = [i for i in range(1, 20)]
test_size_list = [i/100 for i in range(20, 41)]

top_accuracy_score = 0
top_test_size = 0
top_max_depth = 0
top_min_samples_split = 0
top_random_state = 0

for test_size in test_size_list:
  for max_depth in max_depth_list:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
    tfidf.fit(X_train)
    X_train = tfidf.transform(X_train)
    X_test = tfidf.transform(X_test)

    model_decision_tree = DecisionTreeClassifier(
      criterion="entropy", random_state=42, max_depth = max_depth
    )
    model_decision_tree.fit(X_train, y_train)
    y_pred = model_decision_tree.predict(X_test)

    if accuracy_score(y_test, y_pred) > top_accuracy_score:
      top_accuracy_score = accuracy_score(y_test, y_pred)
      top_test_size = test_size
      top_max_depth = max_depth

print("akurasi tertinggi: ", top_accuracy_score)
print("max depth        : ", top_max_depth)
print("test size        : ", top_test_size)